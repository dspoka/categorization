{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('results_tensor2.csv','r')\n",
    "X_data = []\n",
    "y_data = []\n",
    "for line in f:\n",
    "    data = line.strip().split(',')\n",
    "    X_data.append(data[:56])\n",
    "    y_data.append(data[56:])\n",
    "X_train = X_data[:5000]\n",
    "y_train = y_data[:5000]\n",
    "X_test = X_data[5000:]\n",
    "y_test = y_data[5000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10/10 [==============================] - 0s - loss: 0.0995 - acc: 0.9000     \n",
      "10/10 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.layers import Merge, merge\n",
    "from keras.layers import Input\n",
    "\n",
    "\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=numpy.nan)\n",
    "\n",
    "eng_num_preps = 14\n",
    "ger_num_preps = 10\n",
    "PCA_dimension = 56\n",
    "language_embedding_dimension = 28\n",
    "\n",
    "eng_num_preps = 2\n",
    "ger_num_preps = 3\n",
    "PCA_dimension = 3\n",
    "language_embedding_dimension = 4\n",
    "\n",
    "\n",
    "def load_english_data():\n",
    "\t''' 1. seperate train and test better\n",
    "\t\t2. change number of iterations for test/train\n",
    "\t'''\n",
    "\tf = open('results_tensor2.csv','r')\n",
    "\tXY_data=[]\n",
    "\tX_data = []\n",
    "\ty_data = []\n",
    "\tsecond_language = [0 for x in range(ger_num_preps)]\n",
    "\tfor line in f:\n",
    "\t    data = line.strip().split(',') + [0] + second_language\n",
    "\t    XY_data.append(data)\n",
    "\t    # add one to signify what language\n",
    "\t    # 0 = english\n",
    "\t    # 1 = german\n",
    "\t    X_data.append(data[:PCA_dimension+1])\n",
    "\t    y_data.append(data[PCA_dimension+1:])\n",
    "\tX_train = X_data[:5000]\n",
    "\ty_train = y_data[:5000]\n",
    "\tX_test = X_data[5000:]\n",
    "\ty_test = y_data[5000:]\n",
    "\treturn X_train,y_train, X_test, y_test\n",
    "\n",
    "def load_fake_data():\n",
    "\t''' 1. seperate train and test better\n",
    "\t\t2. change number of iterations for test/train\n",
    "\t'''\n",
    "\tf = open('fake_tensor_results.csv','r')\n",
    "\tXY_data=[]\n",
    "\tX_data = []\n",
    "\ty_data = []\n",
    "\tsecond_language = [0 for x in range(ger_num_preps)]\n",
    "\tfor line in f:\n",
    "\t    data = line.strip().split(',') + [0] + second_language\n",
    "\t    XY_data.append(data)\n",
    "\t    # add one to signify what language\n",
    "\t    # 0 = english\n",
    "\t    # 1 = german\n",
    "\t    X_data.append(data[:PCA_dimension+1])\n",
    "\t    y_data.append(data[PCA_dimension+1:])\n",
    "\tX_train = X_data[:10]\n",
    "\ty_train = y_data[:10]\n",
    "\tX_test = X_data[10:]\n",
    "\ty_test = y_data[10:]\n",
    "\treturn X_train,y_train, X_test, y_test\n",
    "\n",
    "def set_fake_pca_weights():\n",
    "\t'''expects a file with weight on a each new line '''\n",
    "\tpca_results = open('results_fake_PCA.csv')\n",
    "\tpca_weights = np.zeros((PCA_dimension+1,PCA_dimension+1))\n",
    "\ti = 0\n",
    "\tfor line in pca_results:\n",
    "\t\ta = float(line.strip())\n",
    "\t\tpca_weights[i][i] = a\n",
    "\t\ti +=1\n",
    "\tpca_weights[i][i] = 1.0\n",
    "\t# unsure what to set the language gate to start at\n",
    "\t# currently set to 1\n",
    "\tbias = np.concatenate((np.zeros(PCA_dimension),np.ones(1)))\n",
    "\treturn [pca_weights,bias]\n",
    "\n",
    "# X_train, y_train, X_test, y_test = load_english_data()\n",
    "#\n",
    "X_train, y_train, X_test, y_test = load_fake_data()\n",
    "\n",
    "# unique_data = [list(x) for x in set(tuple(x) for x in XY_data)]\n",
    "# print len(unique_data)\n",
    "# this shows that there are only a total of 101 unique X-Y pairs\n",
    "\n",
    "def set_pca_weights():\n",
    "\t'''expects a file with weight on a each new line '''\n",
    "\tpca_results = open('results_PCA.csv')\n",
    "\tpca_weights = np.zeros((PCA_dimension+1,PCA_dimension+1))\n",
    "\ti = 0\n",
    "\tfor line in pca_results:\n",
    "\t\ta = float(line.strip())\n",
    "\t\tpca_weights[i][i] = a\n",
    "\t\ti +=1\n",
    "\tpca_weights[i][i] = 1.0\n",
    "\t# unsure what to set the language gate to start at\n",
    "\t# currently set to 1\n",
    "\tbias = np.concatenate((np.zeros(PCA_dimension),np.ones(1)))\n",
    "\treturn [pca_weights,bias]\n",
    "\n",
    "# _______________first layer___________________#\n",
    "# language_input = Sequential()\n",
    "language_input = Input(shape=(PCA_dimension+1,))\n",
    "# gets the pca over all languages to initialize gates to\n",
    "\n",
    "# gate_weights = set_pca_weights()\n",
    "gate_weights = set_fake_pca_weights()\n",
    "\n",
    "gate_layer = Dense(PCA_dimension+1, activation='linear', input_dim=PCA_dimension+1, weights=gate_weights)(language_input)\n",
    "# _____________________________________________#\n",
    "\n",
    "# _____________English Middle Layer____________#\n",
    "english_representation = Dense(language_embedding_dimension,activation='linear', init='uniform')(gate_layer)\n",
    "\n",
    "# _____________________________________________#\n",
    "\n",
    "# _____________English Model_____________#\n",
    "english_predictions = Dense(eng_num_preps+ger_num_preps, init='uniform', activation='softmax')(english_representation)\n",
    "english_model = Model(name='english', input=language_input, output=english_predictions)\n",
    "# _____________________________________________#\n",
    "\n",
    "\n",
    "# ____________Optimization & Training___________#\n",
    "sgd = SGD(lr=0.1, decay=1e-6)\n",
    "english_model.compile(loss='mean_squared_error',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "english_model.fit(X_train, y_train,\n",
    "          nb_epoch=1,\n",
    "          batch_size=1)\n",
    "score = english_model.evaluate(X_test, y_test, batch_size=10)\n",
    "# _____________________________________________#\n",
    "\n",
    "\n",
    "german_representation = Dense(language_embedding_dimension,activation='linear', init='uniform')(gate_layer)\n",
    "bilingual_representation = merge([english_representation,german_representation], mode='concat')\n",
    "\n",
    "# german_predictions = Dense(eng_num_preps+ger_num_preps, init='uniform', activation='softmax')(bilingual_representation)\n",
    "\n",
    "# bilingual_predictions = Dense(eng_num_preps+ger_num_preps, weights=english_predictions+german_predictions, activation='softmax')\n",
    "bilingual_predictions= Dense(eng_num_preps+ger_num_preps, init='uniform', activation='softmax')(bilingual_representation)\n",
    "bilingual_model = Model(name='bilingual', input=language_input, output=bilingual_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__cmp__', '__delattr__', '__doc__', '__format__', '__func__', '__get__', '__getattribute__', '__hash__', '__init__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__self__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'im_class', 'im_func', 'im_self']\n"
     ]
    }
   ],
   "source": [
    "print dir(english_predictions.transfer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_weights(models):\n",
    "\tfor model in models:\n",
    "\t\tweights = model.get_weights()\n",
    "\t\tfilename = 'keras_' + model.name + '_weights.txt'\n",
    "\t\tg = open(filename,'w')\n",
    "\t\tg.write(str(weights))\n",
    "\t\tg.close()\n",
    "\n",
    "\t# weights = bilingual_model.get_weights()\n",
    "\t# g = open('keras_bilingual_weights.txt','w')\n",
    "\t# g.write(str(weights))\n",
    "\t# g.close()\n",
    "print_weights([english_model, bilingual_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
